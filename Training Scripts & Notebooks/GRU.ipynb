{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for training GRU cell RNN model to predict energy production in 1/31, 3/31, 5/31, and for our final submission, 7/31. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and create dataframe object from the CSV file\n",
    "df = pd.read_csv('../Data/ProductionData/Weather-Energy_rev_time_hourly_data.csv')\n",
    "\n",
    "# print(df.info())\n",
    "\n",
    "print(df.head())                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['day', 'hour', 'temperature','wind_speed(m/s)','humidity(%)','solar_radiation(MJ/m^2)','energy']\n",
    "# cols = ['day','hour','temperature','wind_speed(m/s)','humidity(%)','solar_radiation(MJ/m^2)','energy']\n",
    "dataset = df.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index = df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['humidity(%)'] = 100 - dataset['humidity(%)']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_y = MinMaxScaler(copy=False)\n",
    "scaler_x = MinMaxScaler(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index where our training data begins\n",
    "\n",
    "start_idx_131 = df[df['time']==\"2019-12-01 0:00\"].index.tolist()[0]\n",
    "end_idx_131 = df[df['time']==\"2020-01-31 0:00\"].index.tolist()[0]\n",
    "\n",
    "start_idx_331 = df[df['time']==\"2020-02-01 0:00\"].index.tolist()[0]\n",
    "end_idx_331 = df[df['time']==\"2020-03-31 0:00\"].index.tolist()[0]\n",
    "\n",
    "start_idx_531 = df[df['time']==\"2020-04-01 0:00\"].index.tolist()[0]\n",
    "end_idx_531 = df[df['time']==\"2020-05-31 0:00\"].index.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we separate our dataset into two numpy arrays. One contains 'features' another contains 'energy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.to_numpy()\n",
    "Y = dataset.loc[:, 'energy'].to_numpy()\n",
    "Y = np.reshape(Y, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve the skewness problem of our data, we scaled values using scikit-learn's preprocessing utility 'MinMaxScaler'. Unfortunately, after training is done and the model gives us the prediction, the inverse scaling operation didn't work. So we had to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_max = np.max(Y)\n",
    "energy_min = np.min(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x.fit_transform(X)\n",
    "scaler_y.fit_transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_train_data(x_seq, y_seq, scaler_x, scaler_y):\n",
    "    \n",
    "#    x_scaled = scaler_x.fit_transform(x_seq)\n",
    "#    y_scaled = scaler_y.fit_transform(y_seq)\n",
    "    \n",
    "    scaler_x.fit_transform(x_seq)\n",
    "    scaler_y.fit_transform(y_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_data_hour(start_idx, end_idx, seq_in_days, target_hrs, stride, data_array, label_array):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    num_data_in_seq = 24 * seq_in_days\n",
    "    \n",
    "    if (target_hrs < 0):\n",
    "        num_data_in_target_hrs = - target_hrs\n",
    "    else:\n",
    "        num_data_in_target_hrs = target_hrs\n",
    "    \n",
    "    train_data_starts_at = start_idx\n",
    "    train_data_ends_at = end_idx - num_data_in_seq - num_data_in_target_hrs\n",
    "    \n",
    "    for i in range(train_data_starts_at, train_data_ends_at, stride):\n",
    "        x_temp = data_array[i:i+num_data_in_seq].tolist()\n",
    "        y_temp = label_array[i+num_data_in_seq:i+num_data_in_seq+num_data_in_target_hrs]\n",
    "        y_temp = np.reshape(y_temp, (num_data_in_target_hrs, 1)).tolist()\n",
    "        \n",
    "        x.append(x_temp)\n",
    "        y.append(y_temp)\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_531 = X[start_idx_531:end_idx_531,:]\n",
    "trainY_531 = Y[start_idx_531:end_idx_531,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data for prediction\n",
    "\n",
    "trainX_531, trainY_531 = gen_train_data_hour(start_idx_531, end_idx_531, 4, 24, 96, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The very first estimations are not accurate, and it can corrupt the overall performance of the network\n",
    "# So we ignore the first 50 results during the training\n",
    "warmup_steps = 50\n",
    "\n",
    "def loss_mse_warmup(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the MSE between y_true and y_pred\n",
    "    Ignore the beginning 'warmup' steps of the sequences\n",
    "\n",
    "    :param y_true: desired output\n",
    "    :param y_pred: prediction made by model\n",
    "    :return: MSE between answer and prediction while ignoring the results from early stage\n",
    "    \"\"\"\n",
    "\n",
    "    y_true_slice = y_true[:, warmup_steps:, :]\n",
    "    y_pred_slice = y_pred[:, warmup_steps:, :]\n",
    "\n",
    "    loss = tf.losses.MSE(y_true_slice, y_pred_slice)\n",
    "    # It's unclear whether Keras reduce a tensor of losses to a scalar value or not.\n",
    "    # To ensure clarity, it's better to calculate the mean of the losses and return it.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules related to build a RNN\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import GRU, Dense, RepeatVector, TimeDistributed\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(units=128, activation='relu', return_sequences=True, input_shape=trainX_531.shape[-2:]))\n",
    "model.add(GRU(units=64, activation='relu', return_sequences=False))\n",
    "model.add(RepeatVector(trainY_531.shape[1]))\n",
    "model.add(GRU(units=64, activation='relu', return_sequences=True))\n",
    "model.add(GRU(units=128, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=1e-3)     # learning rate = 0.001\n",
    "\n",
    "model.compile(loss=losses.mean_squared_error, optimizer=optimizer, metrics=[custom_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-4, patience=0, verbose=1)\n",
    "\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into : train, test\n",
    "\n",
    "x_train, y_train, x_val, y_val = split_data(trainX_531, trainY_531, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=64,epochs=30, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gru_531_20200727.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../Data/Model/gru_531_20200727.h5', custom_objects = {'custom_loss' : custom_loss })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_531 = model.predict(np.expand_dims(dataset[end_idx_531-96:end_idx_531], axis = 0))\n",
    "pred_531 = np.reshape(pred_531, pred_531.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_531)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually restore the data\n",
    "\n",
    "restored_vals = (energy_max - energy_min) * pred_531 + energy_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(restored_vals, label='Prediction of energy production on 5/31')\n",
    "plt.legend()\n",
    "plt.plot()\n",
    "\n",
    "# %% [markdown]\n",
    "# TimeSeries Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_iter = []\n",
    "loss_per_iter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = trainX_531\n",
    "Y_seq = trainY_531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 1\n",
    "for train_idx, test_idx in tscv.split(X_seq):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=128, activation='relu', return_sequences=True, input_shape=trainX_531.shape[-2:]))\n",
    "    model.add(GRU(units=64, activation='relu', return_sequences=False))\n",
    "    model.add(RepeatVector(trainY_531.shape[1]))\n",
    "    model.add(GRU(units=64, activation='relu', return_sequences=True))\n",
    "    model.add(GRU(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "    optimizer = optimizers.RMSprop(lr=1e-3)     # learning rate = 0.001\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=losses.mean_squared_error, optimizer=optimizer, metrics=[custom_loss])\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for {iter_num} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(X_seq[train_idx], Y_seq[train_idx],\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              verbose=verbosity)\n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(X_seq[test_idx], Y_seq[test_idx], verbose=0)\n",
    "    print(f'Score for iteration {iter_num}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_iter.append(scores[1] * 100)\n",
    "    loss_per_iter.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    iter_num = iter_num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per iteration')\n",
    "for i in range(0, len(acc_per_iter)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_iter[i]} - Accuracy: {acc_per_iter[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_iter)} (+- {np.std(acc_per_iter)})')\n",
    "print(f'> Loss: {np.mean(loss_per_iter)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesplit_res = model.predict(np.expand_dims(X[end_idx_531-96:end_idx_531], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesplit_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesplit_res = np.reshape(timesplit_res, timesplit_res.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timesplit_res, label='5/31')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
