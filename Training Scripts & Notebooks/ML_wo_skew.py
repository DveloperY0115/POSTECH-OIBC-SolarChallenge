# -*- coding: utf-8 -*-
"""solar energy regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1niFukkRbyataw6fvEG3P2y7AYtx0eBr0
"""

# Essentials
import numpy as np
import pandas as pd
import datetime
import random

# Plots
import seaborn as sns
import matplotlib.pyplot as plt

# Models
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.svm import SVR
from mlxtend.regressor import StackingCVRegressor
import lightgbm as lgb
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor

# Stats
from scipy import stats
from scipy.stats import skew, norm
from scipy.special import boxcox1p
from scipy.stats import boxcox_normmax

# Misc
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import scale
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA

pd.set_option('display.max_columns', None)

train = pd.read_csv('./Weather-Energy_rev_data.csv')
test = pd.read_csv('./Weather-Energy_rev_data.csv')

print(train.head())

train.info()

sns.distplot(train['energy'],fit=norm);

plt.ylabel('Frequency')
plt.title('energy distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(train['energy'], plot=plt)
plt.show()

plt.figure(figsize=(15,15))
sns.heatmap(train.corr(),cmap='coolwarm',annot = True)
plt.show()

corr = train.corr()
# drop columns with correlations less than 0.3
cols = corr[abs(corr['energy'])>0.7].index.tolist()
train = train[cols]

sns.lmplot(x='solar_radiation(MJ/m^2)',y='energy',data=train)

total = train.isnull().sum().sort_values(ascending=False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(25)

train.isnull().sum().sort_values(ascending=False).head(20)

# train['wind_speed(m/s)']=train['wind_speed(m/s)'].fillna(method='bfill')
# train['temperature']=train['temperature'].fillna(method='bfill')
# train['humidity(%)']=train['humidity(%)'].fillna(method='bfill')
# train['humidity(%)']= -train['humidity(%)']
df = train
train=train.dropna(axis=0)

train.isnull().sum().sort_values(ascending=False).head(20)

np.where(np.isnan(train))

fig, ax= plt.subplots(1,2, figsize = (10,5))
print("Before >  Mean: %f, Standard Deviation: %f" %norm.fit(train['energy']))
sns.distplot(train['energy'], ax = ax[0])
stats.probplot(train['energy'], plot=ax[1])
plt.show()

print("Skewness: %f" % train['energy'].skew())
print("Kurtosis: %f" % train['energy'].kurt())

# train["energy"] = np.log1p(train["energy"])
# handle skewed data

print("Skewness: %f" % train['energy'].skew())
print("Kurtosis: %f" % train['energy'].kurt())

sns.set_style("white")
sns.set_color_codes(palette='deep')
f, ax = plt.subplots(figsize=(8, 7))
#Check the new distribution 
sns.distplot(train['energy'] , fit=norm, color="b");

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(train['energy'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
ax.xaxis.grid(False)
ax.set(ylabel="Frequency")
ax.set(xlabel="Energy")
ax.set(title="Energy distribution")
sns.despine(trim=True, left=True)

k = 15 #number of variables for heatmap
plt.figure(figsize=(16,8))
corrmat = train.corr()
# picking the top 15 correlated features
cols = corrmat.nlargest(k, 'energy')['energy'].index
cm = np.corrcoef(train[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

# Fetch all numeric features
numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numeric = []
for i in train.columns:
    if train[i].dtype in numeric_dtypes:
        numeric.append(i)

# Create box plots for all numeric features
sns.set_style("white")
f, ax = plt.subplots(figsize=(8, 7))
ax.set_xscale("log")
ax = sns.boxplot(data=train[numeric] , orient="h", palette="Set1")
ax.xaxis.grid(False)
ax.set(ylabel="Feature names")
ax.set(xlabel="Numeric values")
ax.set(title="Numeric Distribution of Features")
sns.despine(trim=True, left=True)

# Find skewed numerical features
skew_features = train[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)

high_skew = skew_features[skew_features > 0.7]
skew_index = high_skew.index

print("There are {} numerical features with Skew > 0.7 :".format(high_skew.shape[0]))
skewness = pd.DataFrame({'Skew' :high_skew})
skew_features.head(high_skew.shape[0])

# Normalize skewed features by boxcox transformation
# for i in skew_index:
#     train[i] = boxcox1p(train[i], boxcox_normmax(train[i] + 1))
#     df[i] = boxcox1p(df[i], boxcox_normmax(df[i] + 1))

skew_features = train[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)
skew_features.head()

sns.set_style("white")
f, ax = plt.subplots(figsize=(8, 7))
ax.set_xscale("log")
ax = sns.boxplot(data=train[numeric] , orient="h", palette="Set1")
ax.xaxis.grid(False)
ax.set(ylabel="Feature names")
ax.set(xlabel="Numeric values")
ax.set(title="Numeric Distribution of Features")
sns.despine(trim=True, left=True)

np.where(np.isnan(train))

# Add feature^2 to our data.
# We do this manually, because ML models won't be able to reliably tell if feature^2 is a predictor of energy.

def squares(res, ls):
    m = res.shape[1]
    for l in ls:
        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   
        res.columns.values[m] = l + '_sq'
        m += 1
    return res 

# squared_features = ['humidity(%)','temperature', 'wind_speed(m/s)', 'solar_radiation(MJ/m^2)',
#        'solar_energy(Sec)']
# train = squares(train, squared_features)
# df = squares(df, squared_features)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(train.drop('energy', axis=1), train['energy'], test_size=0.2, random_state=101)

# we are going to scale to data

y_train= y_train.values.reshape(-1,1)
y_test= y_test.values.reshape(-1,1)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.fit_transform(X_test)
y_train = sc_X.fit_transform(y_train)
y_test = sc_y.fit_transform(y_test)

X_test.shape

from sklearn import metrics

def show_predict(y_test,predictions):
    plt.figure(figsize=(15,8))
    plt.scatter(y_test,predictions)
    plt.xlabel('Y Test')
    plt.ylabel('Predicted Y')
    plt.show()

    plt.figure(figsize=(32,16))
    plt.plot(y_test,label ='Test')
    plt.plot(predictions, label = 'predict')
    plt.show()

def show_errors(y_test,predictions):
    print('MAE:', metrics.mean_absolute_error(y_test, predictions))
    print('MSE:', metrics.mean_squared_error(y_test, predictions))
    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))

def rmse(y_test,predictions):
    return np.sqrt(metrics.mean_squared_error(y_test, predictions))

scores = {}

# Linear Regression
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)

predictions = lm.predict(X_test)
predictions= predictions.reshape(-1,1)

show_errors(y_test,predictions)
scores['lin'] = rmse(y_test,predictions)

# Gradient Boosting Regression
from sklearn import ensemble
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error, r2_score

params = {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 2,
          'learning_rate': 0.05, 'loss': 'ls'}
gbr = ensemble.GradientBoostingRegressor(**params)

gbr.fit(X_train, y_train)

gbr_pred=gbr.predict(X_test)
gbr_pred= gbr_pred.reshape(-1,1)

show_errors(y_test,gbr_pred)
scores['gbr'] = rmse(y_test,gbr_pred)

# Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor
dtreg = DecisionTreeRegressor(random_state = 100)
dtreg.fit(X_train, y_train)

dtr_pred = dtreg.predict(X_test)
dtr_pred= dtr_pred.reshape(-1,1)

show_errors(y_test,dtr_pred)
scores['dtr'] = rmse(y_test,dtr_pred)

# Support Vector Machine Regression
from sklearn.svm import SVR
svr = SVR(kernel = 'rbf')
svr.fit(X_train, y_train)

svr_pred = svr.predict(X_test)
svr_pred= svr_pred.reshape(-1,1)

show_errors(y_test,svr_pred)
scores['svr'] = rmse(y_test,svr_pred)

# Random Forest Regression
from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor(n_estimators = 100, random_state = 0)
rfr.fit(X_train, y_train)

rfr_pred= rfr.predict(X_test)
rfr_pred = rfr_pred.reshape(-1,1)

show_errors(y_test,rfr_pred)
scores['rfr'] = rmse(y_test,rfr_pred)

# Model with regularization: Ridge, Lasso, ElasticNet
from sklearn.linear_model import Ridge, Lasso, ElasticNet

from sklearn.metrics import mean_absolute_error, mean_squared_error
## Assiging different sets of alpha values to explore which can be the best fit for the model. 
alpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]
ridge_rmse_log = {}
lasso_rmse_log = {}
elastic_rmse_log = {}
for i in alpha_ridge:
    ## Assigin each model. 
    ridge = Ridge(alpha= i, normalize=True)
    lasso = Lasso(alpha= i, normalize=True)
    elastic = ElasticNet(alpha= i, normalize=True)
    ## fit the model. 
    ridge.fit(X_train, y_train)
    lasso.fit(X_train, y_train)
    elastic.fit(X_train, y_train)
    ## Predicting the target value based on "Test_x"
    ridge_pred = ridge.predict(X_test)
    lasso_pred = lasso.predict(X_test)
    elastic_pred = elastic.predict(X_test)

    ridge_rmse = rmse(y_test, ridge_pred)
    ridge_rmse_log[i]= ridge_rmse
    lasso_rmse = rmse(y_test,lasso_pred)
    lasso_rmse_log[i]= lasso_rmse
    elastic_rmse = rmse(y_test,elastic_pred)
    elastic_rmse_log[i] = elastic_rmse

# Light GBM
import lightgbm as lgb
model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.1, n_estimators=1000,
                              max_bin = 55, bagging_fraction = 0.6,
                              bagging_freq = 6, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =5, min_sum_hessian_in_leaf = 11)


model_lgb.fit(X_train,y_train)

lgb_pred = model_lgb.predict(X_test)
lgb_pred = lgb_pred.reshape(-1,1)

show_errors(y_test,lgb_pred)
scores['lgb']=rmse(y_test,lgb_pred)

min(list(ridge_rmse_log.values()))
model_logs=[ridge_rmse_log,lasso_rmse_log,elastic_rmse_log]
best_i=[]
for model_log in model_logs:
    best_i.append([k for k, v in model_log.items() if v == min(list(model_log.values()))][0])

ridge = Ridge(alpha= best_i[0], normalize=True)
lasso = Lasso(alpha= best_i[1], normalize=True)
elastic = ElasticNet(best_i[2], normalize=True)

ridge.fit(X_train, y_train)
lasso.fit(X_train, y_train)
elastic.fit(X_train, y_train)

ridge_pred = ridge.predict(X_test)
lasso_pred = lasso.predict(X_test)
elastic_pred = elastic.predict(X_test)

print("Ridge")
show_errors(y_test,ridge_pred)
scores['ridge'] = rmse(y_test,ridge_pred)
print("")
print("Lasso")
show_errors(y_test,lasso_pred)
scores['lasso'] = rmse(y_test,lasso_pred)
print("")
print("ElasticNet")
show_errors(y_test,elastic_pred)
scores['elastic'] = rmse(y_test,elastic_pred)

model_xgb = XGBRegressor(learning_rate=0.01,
                       n_estimators=1000,
                       max_depth=4,
                       min_child_weight=0,
                       gamma=0.6,
                       subsample=0.7,
                       colsample_bytree=0.7,
                       objective='reg:squarederror',
                       nthread=-1,
                       scale_pos_weight=1,
                       seed=27,
                       reg_alpha=0.00006,
                       random_state=42)

model_xgb.fit(X_train,y_train)

xgb_pred = model_xgb.predict(X_test)
xgb_pred = xgb_pred.reshape(-1,1)

show_errors(y_test,xgb_pred)
scores['xgb'] = rmse(y_test,xgb_pred)

# Plot the predictions for each model
sns.set_style("white")
fig = plt.figure(figsize=(24, 12))

ax = sns.pointplot(x=list(scores.keys()), y=[score for score in scores.values()], markers=['o'], linestyles=['-'])
for i, score in enumerate(scores.values()):
    ax.text(i, score, '{:.6f}'.format(score), horizontalalignment='left', size='large', color='black', weight='semibold')

plt.ylabel('Score (RMSE)', size=20, labelpad=12.5)
plt.xlabel('Model', size=20, labelpad=12.5)
plt.tick_params(axis='x', labelsize=13.5)
plt.tick_params(axis='y', labelsize=12.5)

plt.title('Scores of Models', size=20)

plt.show()

def blend_models_predict(X):
    models = [lm, svr, gbr, rfr, model_lgb,ridge,lasso,elastic]
    weights = [0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0]
    i = 0
    total = 0
    for model in models:
        temp = weights[i]*model.predict(X)
        temp = temp.reshape(-1,1)
        total += temp
    return total

original = pd.read_csv('./Weather-Energy_rev_data.csv')

start = original[original['time']=="2020-03-30 0:00"].index.tolist()[0]
end = original[original['time']=="2020-03-31 0:00"].index.tolist()[0]

answer = original['energy']
answer = answer[start:end]

test1 = df[start:end]
test1 = test1.drop('energy', axis=1)
test1 = sc_X.fit_transform(test1)

answer.to_numpy()

test_prediction = blend_models_predict(test1)

test_prediction = test_prediction.reshape(-1,1)
test_prediction = sc_y.inverse_transform(test_prediction)


# test_prediction = np.expm1(test_prediction)

for i in range(len(test_prediction)):
    if test_prediction[i]<0:
        test_prediction[i]=0


from sklearn.metrics import mean_squared_error
mse = mean_squared_error(answer, test_prediction)


final_data=[]
for i in range(0, len(test_prediction), 4):
    final_data.append(test_prediction[i:i+4].sum())
